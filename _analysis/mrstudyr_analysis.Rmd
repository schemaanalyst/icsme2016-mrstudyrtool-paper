---
title: "MRSTUDYR: Retrospectively Studying the Effectiveness of Mutant Reduction Techniques"
output:
  pdf_document:
    keep_tex: true
    highlight: pygments
    number_sections: true
---

# Installing the Ineffective Mutant Analysis Package

This code will install the ineffectivemutants package from GitHub using the `install_github` function.

```{r, echo=TRUE}
options(scipen=10, width=200)
devtools::install_github("mccurdyc/mrstudyr")
```

> # Comparing the Mutation Scores

## Initialise the System

First, load in the libraries that are used in addition to those with the mrstudyr package (i.e., load all
of the packages not used by mrstudyr but still used in this RMarkdown file). Note that right now the
mrstudyr package will automatically load all of the packages that it needs to performs its various
analyses. Now, we are ready to call the functions from the ineffectivemutants package and produce the appropriate
summary tables and graphs.

```{r, echo=TRUE}
suppressPackageStartupMessages(library(mrstudyr))
suppressPackageStartupMessages(library(knitr))
```

## Show the Schemas for the ICSME 2016 Paper

```{r, echo=TRUE}
sqlite_data <- read_sqlite_avmdefaults()
schemas <- sqlite_data %>% collect_study_schemas() %>% collect_normal_data() %>% select(schema) %>% unique()
knitr::kable(schemas, format="latex", booktabs=TRUE)
```

## Perform Reduction Techniques

First, performing the most common reduction technique, random sampling.

```{r, echo=TRUE}
d <- read_sqlite_avmdefaults()
normal_data <- d %>% collect_study_schemas() %>% collect_normal_data()
random_sampling_data <- d %>% analyse_random_sampling()
knitr::kable(head(random_sampling_data), format="latex", booktabs=TRUE)
```

Next, applying the concepts of random sampling but, over operators.

```{r, echo=TRUE}
d <- read_sqlite_avmdefaults()
normal_data <- d %>% collect_study_schemas() %>% collect_normal_data()
operator_sampling_data <- d %>% analyse_operator_sampling()
knitr::kable(head(operator_sampling_data), format="latex", booktabs=TRUE)
```
## Visualise the Mutation Scores for the Random Sampling Technique

Now, plot the graph that shows the mutation score for the random sampling mutant reduction technique for four user-specified percentages.
In this example, we will create a visualisation containing the percentages
found in the accompanying paper.

```{r, mutation_score_random, fig.width=5.7, fig.height=3.5, echo=TRUE}
d <- read_sqlite_avmdefaults()
normal_data <- d %>% collect_study_schemas() %>% collect_normal_data()
filtered_percents_data <- random_sampling_data %>% collect_chosen_percent_data(c(1, 10, 20, 40))
visualise_random_sampling_mutation_scores(filtered_percents_data)
```

Now, plot the graph that shows the mutation score for the operator sampling mutant reduction technique.

```{r, mutation_score_operator, fig.width=5.7, fig.height=3.5, echo=TRUE}
d <- read_sqlite_avmdefaults()
normal_data <- d %>% collect_study_schemas() %>% collect_normal_data()
filtered_percents_data <- operator_sampling_data %>% collect_chosen_percent_data(c(1, 10, 20, 40))
visualise_operator_sampling_mutation_scores(filtered_percents_data)
```
Analysis of the results in this graph:

- Much more variability for the time-constrained method than the virtual one
- However, the mutation scores are often roughly similar between the two techniques, especially for the HyperSQL and
  SQLite DBMSs
- It is important to note that the time-constrained method yields mutation scores that are higher than those produced by
  the virtual mutation technique (e.g., CoffeeOrders and Employee schemas). And, note that the virtual method
  leads to a score that is the same as the standard one. So, time constrained will over estimate the scores sometimes.
- Greater variability in the Person schema is due to the fact that sometimes the mutation score is zero (i.e., all of
  the mutants are killed) and other times it is one (i.e., none of the mutants are killed)

Additional data points to support this analysis:

```{r, echo=TRUE}
time_constrained_data_tidy_print <- time_constrained_data_tidy %>%
  filter(schema %in% c("Person")) %>%
  filter(dbms %in% c("PostgreSQL")) %>%
  filter(score_type %in% c("Time-Constrained"))
knitr::kable(time_constrained_data_tidy_print, format="latex", booktabs=TRUE)
```
Questions about the results analysis:

- Is there every a case where we want to do statistical analysis or effect size calculations?
- I think that the "transformation" idea from Harman would apply to our analysis of the timing values.
